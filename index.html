<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="When Words SmileðŸ˜€: Generating Diverse Emotional Facial Expressions from Text">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>EmoAva</title> 

    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <link rel="stylesheet" href="./static/css/index-gradio.css">
    <link rel="stylesheet" href="./static/css/live_theme.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <!-- <script src="./static/js/index.js"></script> -->
  <style>
    .video-container {
      display: flex;
      justify-content: space-between;
      gap: 20px;
      padding: 20px;
      flex-wrap: nowrap;
    }

    .video-item {
      text-align: center;
      flex: 1;
      max-width: 25%;
      position: relative;
    }

    .video-wrapper {
      position: relative;
      width: 100%;
    }

    .video-wrapper video {
      width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }

    .cover {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: #000;
      border-radius: 8px;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .cover img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      border-radius: 8px;
    }

    .cover::after {
      content: "â–¶";
      position: absolute;
      font-size: 16px;
      color: white;
      background: rgba(0,0,0,0.5);
      width: 50px;
      height: 50px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 2;
      left: 10px;      
      bottom: 10px;   
      top: auto;       
      right: auto;     
    }

    .video-item p {
      margin-top: 8px;
      font-size: 14px;
      color: #333;
    }
  </style>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title"
                        style="display: flex;flex-direction: row;align-items: center;justify-content: center;margin-bottom: 5px;">
                        When Words SmileðŸ˜€:</h1>
                    <h1 class="title is-2 publication-title">Generating Diverse Emotional Facial Expressions from Text</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Haidong Xu</a>,</span>
                        <span class="author-block">
              <a href="https://zhangmeishan.github.io/">Meishan Zhang</a>,</span>
                        <span class="author-block">
              <a href="#">Hao Ju</a>,</span>
                        <span class="author-block">
              <a href="https://www.zdzheng.xyz/">Zhedong Zheng</a>,</span>
                        <span class="author-block">
            </span>
                        <span class="author-block">
              <a href="https://sentic.net/">Erik Cambria</a>,</span>
                        <span class="author-block">
              <a href="https://zhangmin-nlp-ai.github.io/">Min Zhang</a>,</span>
                        <span class="author-block">
              <a href="https://haofei.vip/">Hao Fei*</a>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors" style="margin-top: 10px;">
                        <span class="author-block">Harbin Institute of Technology (Shenzhen)</a>, University of Macau,</span>
                    </div>
                    <div class="is-size-5 publication-authors" style="margin-top: 10px;">
                        <span class="author-block">Nanyang Technological University, National University of Singapore</span>
                    </div>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block" style="font-size: 15px;">EMNLP 2025, Oral (<sup>*</sup>Correspondence)</span>
                    </div>


                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.02508"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

                            <!-- <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-laugh"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span> -->

                            <!-- Code Link. -->
                            <span class="link-block">
                <a href="https://github.com/WalkerMitty/EmoAva"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>


                            <span class="link-block">
                <a href="https://github.com/WalkerMitty/EmoAva?tab=readme-ov-file#dataset-recipe" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>

                            <!-- Video Link. -->
                            <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->


                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>




<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h3 class="title is-3">Abstract</h3>
                <div class="content has-text-justified">
                    <p>
                        Enabling digital humans to express rich emotions has significant applications in dialogue
                        systems, gaming, and other interactive scenarios. While recent advances in talking head
                        synthesis have achieved impressive results in
                        lip synchronization, they tend to overlook the
                        rich and dynamic nature of facial expressions.
                        To fill this critical gap, we introduce an endto-end text-to-expression model that explicitly
                        focuses on emotional dynamics. Our model learns expressive facial variations in a continuous latent space and generates expressions that
                        are diverse, fluid, and emotionally coherent.
                        To support this task, we introduce EmoAva,
                        a large-scale and high-quality dataset containing 15,000 textâ€“3D expression pairs. Extensive experiments on both existing datasets and
                        EmoAva demonstrate that our method significantly outperforms baselines across multiple
                        evaluation metrics, marking a significant advancement in the field.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

        <br>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">

        <div class="columns is-centered has-text-centered">
            <h3 class="title is-3">OverView</h3>
            <br>
        </div>

        <!-- Architecture -->
        <div class="columns is-centered">
            <div class="column is-full-width">

                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/introduction4_01.png" alt="Teaser" width="70%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: left; color: #061E61;">
                            <b>Top:</b> The existing pipeline for synthesizing
                            emotional avatars, which can only generate limited expressions that lack of diversity. 
                            <b>Bottom:</b> The proposed
                            end-to-end system that directly maps text to facial expressions (codes), aims to generate diverse, emotionally
                            consistent, and temporally smooth expressions.
                        </p>
                    </figcaption>
                    <br>
                </div>
                <br/>

            </div>
        </div>

    

    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">

        <div class="columns is-centered has-text-centered">
            <h3 class="title is-3">Demonstrations</h3>
            <br>
        </div>
  <div class="video-container">
    
    <div class="video-item">
        <div class="video-wrapper">
      <video controls muted>
        <source src="./static/videos/so_dead.mp4" type="video/mp4">
      </video>
        <div class="cover" onclick="playVideo(this)">
          <img src="./static/videos/frame_dead.png" alt="">
        </div>
        </div>
      <p>Text: I am so dead.</p>
    </div>

    <div class="video-item">
        <div class="video-wrapper">
      <video controls muted>
        <source src="./static/videos/be_great.mp4" type="video/mp4">
      </video>
        <div class="cover" onclick="playVideo(this)">
          <img src="./static/videos/frame_be_great.png" alt="">
        </div>
        </div>
      <p>Text: That'd be great!</p>
     
    </div>

    <div class="video-item">
        <div class="video-wrapper">
      <video controls muted>
        <source src="./static/videos/beautiful_story.mp4" type="video/mp4">
      </video>
        <div class="cover" onclick="playVideo(this)">
          <img src="./static/videos/frame_beautiful.png" alt="">
        </div>
        </div>
      <p>Text: What a beautiful story.</p>
    </div>

    <div class="video-item">
         <div class="video-wrapper">
      <video controls muted>
        <source src="./static/videos/the_hell.mp4" type="video/mp4">
      </video>
        <div class="cover" onclick="playVideo(this)">
          <img src="./static/videos/frame_hell.png" alt="">
        </div>
        </div>
      <p>Text: What the hell?</p>
    </div>
  </div>
  <script>
    function playVideo(coverDiv) {
  
      const video = coverDiv.previousElementSibling;


      video.playbackRate = 1.0;

      video.play().then(() => {
 
        coverDiv.style.display = 'none';
      }).catch(err => {
        console.log("play fails:", err);
   
        video.controls = true;
      });
    }

    document.querySelectorAll('video').forEach(v => {
      v.preload = 'metadata';
    });
  </script>

    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">

        <div class="columns is-centered has-text-centered">
            <h3 class="title is-3">Continuous Text-to-Expression Generator</h3>
            <br>
        </div>

        <!-- Architecture -->
        <div class="columns is-centered">
            <div class="column is-full-width">

                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/method1-v3_01.png" alt="Teaser" width="50%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: left; color: #061E61;">
                            Given a text, the model
                            autoregressively generates a sequence of expression
                            vectors. The green block and pink block represent
                            the proposed Expression-wise Attention (EwA) module
                            and the core Conditional Variational Autoregressive Decoder (CVAD) module, respectively.
                        </p>
                    </figcaption>
                    <br>
                </div>
                <br/>

            </div>
        </div>

    

    </div>
</section>

<!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
</code></pre>
    </div>
</section> -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p style="text-align: center;">
                        The webpage is built based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
